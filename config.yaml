---
general:
    pad_token: 0
    sos_token: 1
    eos_token: 2
    unk_token: 3
    datasets_directory: "local_datasets"
    tokenizers_directory: "local_tokenizers"

datasets:
    - gigaword:
        name: "gigaword"
        force_download: false
        num_workers: 4
        trust_remote_code: false
        stream: false
    - billsum:
          name: "billsum"
          force_download: false
          num_workers: 4
          trust_remote_code: false
          stream: false

tokenizers:
    - sentencepiece:
        model_name: "spm_m"
        model_version: "1_0"
        vocab_size: 32000
        force_train: false
        num_threads: 12
        training_file: "temp/gigaword.txt"
        input_sentence_size: 0
        shuffle_input_sentence: true

models:
    - transformer:
        model_dim: 512
        inner_ff_dim: 2048
        n_encoders: 6
        n_decoders: 6

optimizers:
    - adamW:
        learning_rate: 1e-4
        beta_1: 0.9
        beta_2: 0.98
        eps: 1e-9

main_params:
    epochs: 1
    batch_size: 128
    device: "cuda"
    tokenizer: "sentencepiece"
    train_status_print: 20
    validation_status_print: 20
    model: "transformer"
    optimizer: "adamW"
    grad_norm: 1.0
    max_len: 1000
    checkpoint_path: ""
